{"cells":[{"cell_type":"markdown","metadata":{"id":"A6czvz5VKO5M"},"source":["# Notebook for Programming in Problem 2\n"]},{"cell_type":"markdown","metadata":{"id":"5o8HI5JqTvU5"},"source":["## Learning Objectives\n","In this problem, we will use [PyTorch](https://pytorch.org/) to implement long short-term memory (LSTM) for named entity recognition (NER). We will use the same dataset and boilerplate code as in Programming Problem 1 of Assignment #3."]},{"cell_type":"markdown","metadata":{"id":"ObrHyvWvTyGZ"},"source":["## Writing Code\n","Look for the keyword \"TODO\" and fill in your code in the empty space.\n","Feel free to change function signatures, but be careful that you might need to also change how they are called in other parts of the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r6YTnpgbFdMI"},"outputs":[],"source":["!nvidia-smi # you may need to try reconnecting to get a T4 gpu"]},{"cell_type":"markdown","metadata":{"id":"tnYMKJlKNXYe"},"source":["## Installing PyTorch and Other Packages\n","\n","Install PyTorch using pip. See [https://pytorch.org/](https://pytorch.org/) if you want to install it on your computer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dRVuiP_JVdT"},"outputs":[],"source":["!pip install torch torchtext -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"markdown","metadata":{"id":"TPsFH637OpLy"},"source":["Test if our installation works:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c62StNb2NvKk"},"outputs":[],"source":["import torch\n","\n","# Multiply two matrices on GPU\n","a = torch.rand(100, 200).cuda()\n","b = torch.rand(200, 100).cuda()\n","c = torch.matmul(a, b)\n","\n","print(\"PyTorch successfully installed!\")\n","print(\"Version:\", torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"1qaC8sxcqkGX"},"source":["Also install [scikit-learn](https://scikit-learn.org/stable/). We will use it for calculating evaluation metrics such as accuracy and F1 score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5Y2xB_uqqM9"},"outputs":[],"source":["!pip install -U scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"bhV4CYivRbt4"},"source":["Let's import all the packages at once:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjRM4cCFRh-d"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchtext.vocab import Vocab, vocab\n","import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","import re\n","from collections import Counter\n","from typing import List, Tuple, Dict, Optional, Any"]},{"cell_type":"markdown","metadata":{"id":"Yn1bIPjAN-9V"},"source":["## Long Short Term Memory (LSTM)"]},{"cell_type":"markdown","metadata":{"id":"sJOKIneRTrTH"},"source":["### Data Loading\n","\n","We will use the same dataset for named entity recognition in Assignment #2. First download the data and take a look at the first 50 lines:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWqz7kDxSqeb"},"outputs":[],"source":["!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n","!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n","!cat eng.train | head -n 50"]},{"cell_type":"markdown","metadata":{"id":"YVt1a6nzWsiF"},"source":["Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WnNfOBUYJvVW"},"outputs":[],"source":["# A sentence is a list of (word, tag) tuples.\n","# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n","Sentence = List[Tuple[str, str]]\n","\n","\n","def read_data_file(\n","    datapath: str,\n",") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n","    \"\"\"\n","    Read and preprocess input data from the file `datapath`.\n","    Example:\n","    ```\n","        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n","    ```\n","    Return values:\n","        `sentences`: a list of sentences, including words and NER tags\n","        `word_cnt`: a Counter object, the number of occurrences of each word\n","        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n","    \"\"\"\n","    sentences: List[Sentence] = []\n","    word_cnt: Dict[str, int] = Counter()\n","    tag_cnt: Dict[str, int] = Counter()\n","\n","    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n","        if \"DOCSTART\" in sentence_txt:\n","            # Ignore dummy sentences at the begining of each document.\n","            continue\n","        # Read a new sentence\n","        sentences.append([])\n","        for token in sentence_txt.split(\"\\n\"):\n","            w, _, _, t = token.split()\n","            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n","            w = re.sub(\"\\d\", \"0\", w)\n","            word_cnt[w] += 1\n","            tag_cnt[t] += 1\n","            sentences[-1].append((w, t))\n","\n","    return sentences, word_cnt, tag_cnt\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLMGYSZ7KxzP"},"outputs":[],"source":["# Some helper code\n","def get_device() -> torch.device:\n","    \"\"\"\n","    Use GPU when it is available; use CPU otherwise.\n","    See https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code\n","    \"\"\"\n","    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVHAOb7iMPwC"},"outputs":[],"source":["def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n","    \"\"\"\n","    Calculate various evaluation metrics such as accuracy and F1 score\n","    Parameters:\n","        `ground_truth`: the list of ground truth NER tags\n","        `predictions`: the list of predicted NER tags\n","    \"\"\"\n","    f1_scores = f1_score(ground_truth, predictions, average=None)\n","    return {\n","        \"accuracy\": accuracy_score(ground_truth, predictions),\n","        \"f1\": f1_scores,\n","        \"average f1\": np.mean(f1_scores),\n","        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n","    }"]},{"cell_type":"markdown","metadata":{"id":"7s830dhbnj1L"},"source":["## Long Short-term Memory (LSTM)\n","\n","Now we implement an one-layer LSTM for the same task and compare it to FFNNs."]},{"cell_type":"markdown","metadata":{"id":"to7DnWNiY5ZS"},"source":["### Data Loading **(4 points)**\n","\n","Like before, we first implement the data loader. But unlike before, each data example is now a variable-length sentence. How can we pack multiple sentences with different lengths into the same batch? One possible solution is to pad them to the same length using a special token. The code below illustrates the idea:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5oVgqE7JaJp"},"outputs":[],"source":["# 3 sentences with different lengths\n","sentence_1 = torch.tensor([6, 1, 2])\n","sentence_2 = torch.tensor([4, 2, 7, 7, 9])\n","sentence_3 = torch.tensor([3, 4])\n","# Form a batch by padding 0\n","sentence_batch = torch.tensor([\n","    [6, 1, 2, 0, 0],\n","    [4, 2, 7, 7, 9],\n","    [3, 4, 0, 0, 0],\n","])"]},{"cell_type":"markdown","metadata":{"id":"udC0SMjkKaCN"},"source":["We implement the above idea in a customized batching function `form_batch`. Optionally, see [here](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data) for how batching works in PyTorch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sACcGN4XYMgj"},"outputs":[],"source":["class SequenceDataset(Dataset):\n","    \"\"\"\n","    Each data example is a sentence, including its words and NER tags.\n","    \"\"\"\n","\n","    def __init__(\n","        self, datapath: str, words_vocab: Optional[Vocab] = None, tags_vocab: Optional[Vocab] = None\n","    ) -> None:\n","        \"\"\"\n","        Initialize the dataset by reading from datapath.\n","        \"\"\"\n","        super().__init__()\n","        self.sentences: List[Sentence] = []\n","        UNKNOWN = \"<UNKNOWN>\"\n","        PAD = \"<PAD>\"  # Special token used for padding\n","\n","        print(\"Loading data from %s\" % datapath)\n","        self.sentences, word_cnt, tag_cnt = read_data_file(datapath)\n","        print(\"%d sentences loaded.\" % len(self.sentences))\n","\n","        if words_vocab is None:\n","            words_vocab = vocab(word_cnt, specials=[PAD, UNKNOWN])\n","            words_vocab.set_default_index(words_vocab[UNKNOWN])\n","\n","        self.words_vocab = words_vocab\n","\n","        self.unknown_idx = self.words_vocab[UNKNOWN]\n","        self.pad_idx = self.words_vocab[PAD]\n","\n","        if tags_vocab is None:\n","            tags_vocab = vocab(tag_cnt, specials=[])\n","        self.tags_vocab = tags_vocab\n","\n","    def __getitem__(self, idx: int) -> Sentence:\n","        \"\"\"\n","        Get the idx'th sentence in the dataset.\n","        \"\"\"\n","        return self.sentences[idx]\n","\n","    def __len__(self) -> int:\n","        \"\"\"\n","        Return the number of sentences in the dataset.\n","        \"\"\"\n","        # TODO: Implement this method\n","        # START HERE\n","        raise NotImplementedError\n","        # END\n","\n","    def form_batch(self, sentences: List[Sentence]) -> Dict[str, Any]:\n","        \"\"\"\n","        A customized function for batching a number of sentences together.\n","        Different sentences have different lengths. Let max_len be the longest length.\n","        When packing them into one tensor, we need to pad all sentences to max_len.\n","        Return values:\n","            `words`: a list in which each element itself is a list of words in a sentence\n","            `word_idxs`: a batch_size x max_len tensor.\n","                       word_idxs[i][j] is the index of the j'th word in the i'th sentence .\n","            `tags`: a list in which each element itself is a list of tags in a sentence\n","            `tag_idxs`: a batch_size x max_len tensor\n","                      tag_idxs[i][j] is the index of the j'th tag in the i'th sentence.\n","            `valid_mask`: a batch_size x max_len tensor\n","                        valid_mask[i][j] is True if the i'th sentence has the j'th word.\n","                        Otherwise, valid[i][j] is False.\n","        \"\"\"\n","        words: List[List[str]] = []\n","        tags: List[List[str]] = []\n","        max_len = -1  # length of the longest sentence\n","        for sent in sentences:\n","            words.append([])\n","            tags.append([])\n","            for w, t in sent:\n","                words[-1].append(w)\n","                tags[-1].append(t)\n","            max_len = max(max_len, len(words[-1]))\n","\n","        batch_size = len(sentences)\n","        word_idxs = torch.full(\n","            (batch_size, max_len), fill_value=self.pad_idx, dtype=torch.int64\n","        )\n","        tag_idxs = torch.full_like(word_idxs, fill_value=self.tags_vocab[\"O\"])\n","        valid_mask = torch.zeros_like(word_idxs, dtype=torch.bool)\n","\n","        ## TODO: Fill in the values in word_idxs, tag_idxs, and valid_mask\n","        ## Caveat: There may be out-of-vocabulary words in validation data\n","        ## See torchtext.vocab.Vocab: https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab\n","        ## START HERE\n","\n","        raise NotImplementedError\n","\n","        # END\n","\n","        return {\n","            \"words\": words,\n","            \"word_idxs\": word_idxs,\n","            \"tags\": tags,\n","            \"tag_idxs\": tag_idxs,\n","            \"valid_mask\": valid_mask,\n","        }\n","\n","\n","def create_sequence_dataloaders(\n","    batch_size: int, shuffle: bool = True\n",") -> Tuple[DataLoader, DataLoader, Vocab]:\n","    \"\"\"\n","    Create the dataloaders for training and validaiton.\n","    \"\"\"\n","    ds_train = SequenceDataset(\"eng.train\")\n","    ds_val = SequenceDataset(\"eng.val\", words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n","    loader_train = DataLoader(\n","        ds_train,\n","        batch_size,\n","        shuffle,\n","        collate_fn=ds_train.form_batch,  # customized function for batching\n","        drop_last=True,\n","        pin_memory=True,\n","    )\n","    loader_val = DataLoader(\n","        ds_val, batch_size, collate_fn=ds_val.form_batch, pin_memory=True\n","    )\n","    return loader_train, loader_val, ds_train"]},{"cell_type":"markdown","metadata":{"id":"E2EcVxYuYvGv"},"source":["Here is a simple sanity-check. Try to understand its output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TazmodGWYx2d"},"outputs":[],"source":["def check_sequence_dataloader() -> None:\n","    loader_train, _, _ = create_sequence_dataloaders(batch_size=3, shuffle=False)\n","    print(\"Iterating on the training data..\")\n","    for i, data_batch in enumerate(loader_train):\n","        if i == 0:\n","            print(data_batch)\n","    print(\"Done!\")\n","\n","\n","check_sequence_dataloader()"]},{"cell_type":"markdown","metadata":{"id":"ifk3i-obY8YB"},"source":["### Implement the Model **(8 points)**\n","\n","Next, implement LSTM for predicting NER tags from input words. [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is definitely useful. Further, it is tricky to handle sentences in the same batch with different lengths. Please read the PyTorch documentation in detail!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3V0NvQynZF8e"},"outputs":[],"source":["class LSTM(nn.Module):\n","    \"\"\"\n","    Long short-term memory for NER\n","    \"\"\"\n","\n","    def __init__(self, words_vocab: Vocab, tags_vocab:Vocab, d_emb: int, d_hidden: int, bidirectional: bool) -> None:\n","        \"\"\"\n","        Initialize an LSTM\n","        Parameters:\n","            `words_vocab`: vocabulary of words\n","            `tags_vocab`: vocabulary of tags\n","            `d_emb`: dimension of word embeddings (D)\n","            `d_hidden`: dimension of the hidden layer (H)\n","            `bidirectional`: true if LSTM should be bidirectional\n","        \"\"\"\n","        super().__init__()\n","        # TODO: Create the word embeddings (nn.Embedding),\n","        #       the LSTM (nn.LSTM) and the output layer (nn.Linear).\n","        #       Read the torch docs for additional guidance : https://pytorch.org/docs/stable\n","        #       Note: Pay attention to the LSTM output shapes!\n","        # START HERE\n","        raise NotImplementedError\n","\n","        # END\n","\n","    def forward(\n","        self, word_idxs: torch.Tensor, valid_mask: torch.Tensor\n","    ) -> torch.Tensor:\n","        \"\"\"\n","        Given words in sentences, predict the logits of the NER tag.\n","        Parameters:\n","            `word_idxs`: a batch_size x max_len tensor\n","            `valid_mask`: a batch_size x max_len tensor\n","        Return values:\n","            `logits`: a batch_size x max_len x 5 tensor\n","        \"\"\"\n","        # TODO: Implement the forward pass\n","        # START HERE\n","        raise NotImplementedError\n","\n","        # END\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"2BFTKaB4Zydx"},"source":["We do a sanity-check by loading a batch of data examples and pass it through the network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKg1ni4QZ6D1"},"outputs":[],"source":["def check_lstm() -> None:\n","    # Hyperparameters\n","    batch_size = 4\n","    d_emb = 64\n","    d_hidden = 128\n","    bidirectional = True\n","    # Create the dataloaders and the model\n","    loader_train, _, ds_train = create_sequence_dataloaders(batch_size)\n","    model = LSTM(ds_train.words_vocab, ds_train.tags_vocab, d_emb, d_hidden, bidirectional)\n","    device = get_device()\n","    model.to(device)\n","    print(model)\n","    # Get the first batch\n","    data_batch = next(iter(loader_train))\n","    # Move data to GPU\n","    word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n","    tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n","    valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n","    # Calculate the model\n","    print(\"Input word_idxs shape:\", word_idxs.size())\n","    print(\"Input valid_mask shape:\", valid_mask.size())\n","    logits = model(word_idxs, valid_mask)\n","    print(\"Output logits shape:\", logits.size())\n","\n","\n","check_lstm()"]},{"cell_type":"markdown","metadata":{"id":"jddDYUiLY-hc"},"source":["### Training and Validation **(6 points)**\n","\n","Complete the functions for training and validating the LSTM model. When calculating the loss function, you only want to include values from valid positions (where `valid_mask` is `True`). The `reduction` parameter in [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy) may be useful."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hv_15mnXZ_dy"},"outputs":[],"source":["def train_lstm(\n","    model: nn.Module,\n","    loader: DataLoader,\n","    optimizer: optim.Optimizer,\n","    device: torch.device,\n","    silent: bool = False,  # whether to print the training loss\n",") -> Tuple[float, Dict[str, Any]]:\n","    \"\"\"\n","    Train the LSTM model.\n","    Return values:\n","        1. the average training loss\n","        2. training metrics such as accuracy and F1 score\n","    \"\"\"\n","    model.train()\n","    ground_truth = []\n","    predictions = []\n","    losses = []\n","    report_interval = 100\n","\n","    for i, data_batch in enumerate(loader):\n","        word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n","        tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n","        valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n","\n","        # TODO: Do the same tasks as train_ffnn\n","        # START HERE\n","        # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n","        raise NotImplementedError\n","        # END\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        losses.append(loss.item())\n","\n","        # we get (unmasked) predictions by getting argmax of logits along last dimension (You will need to define logits!)\n","        net_predictions = torch.argmax(logits, -1)\n","\n","        # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n","        tag_idxs_flat = tag_idxs.flatten()\n","        valid_mask_flat = valid_mask.flatten()\n","        net_predictions_flat = net_predictions.flatten()\n","\n","        ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n","        predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n","\n","        if not silent and i > 0 and i % report_interval == 0:\n","            print(\n","                \"\\t[%06d/%06d] Loss: %f\"\n","                % (i, len(loader), np.mean(losses[-report_interval:]))\n","            )\n","\n","    return np.mean(losses), eval_metrics(ground_truth, predictions)\n","\n","\n","def validate_lstm(\n","    model: nn.Module, loader: DataLoader, device: torch.device\n",") -> Tuple[float, Dict[str, Any]]:\n","    \"\"\"\n","    Validate the model.\n","    Return the validation loss and metrics.\n","    \"\"\"\n","    model.eval()\n","    ground_truth = []\n","    predictions = []\n","    losses = []\n","\n","    with torch.no_grad():\n","\n","        for data_batch in loader:\n","            word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n","            tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n","            valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n","\n","            # TODO: Do the same tasks as validate_ffnn\n","            # START HERE\n","            # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n","            raise NotImplementedError\n","            # END\n","\n","            losses.append(loss.item())\n","\n","            # we get (unmasked) predictions by getting argmax of logits (You will need to define logits!)\n","            net_predictions = torch.argmax(logits, -1)\n","\n","            # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n","            tag_idxs_flat = tag_idxs.flatten()\n","            valid_mask_flat = valid_mask.flatten()\n","            net_predictions_flat = net_predictions.flatten()\n","\n","            ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n","            predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n","\n","    return np.mean(losses), eval_metrics(ground_truth, predictions)\n","\n","\n","def train_val_loop_lstm(hyperparams: Dict[str, Any]) -> None:\n","    \"\"\"\n","    Train and validate the LSTM model for a number of epochs.\n","    \"\"\"\n","    print(\"Hyperparameters:\", hyperparams)\n","    # Create the dataloaders\n","    loader_train, loader_val, ds_train = create_sequence_dataloaders(\n","        hyperparams[\"batch_size\"]\n","    )\n","    # Create the model\n","    model = LSTM(\n","        ds_train.words_vocab,\n","        ds_train.tags_vocab,\n","        hyperparams[\"d_emb\"],\n","        hyperparams[\"d_hidden\"],\n","        hyperparams[\"bidirectional\"],\n","    )\n","    device = get_device()\n","    model.to(device)\n","    print(model)\n","    # Create the optimizer\n","    optimizer = optim.RMSprop(\n","        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n","    )\n","\n","    # Train and validate\n","    for i in range(hyperparams[\"num_epochs\"]):\n","        print(\"Epoch #%d\" % i)\n","\n","        print(\"Training..\")\n","        loss_train, metrics_train = train_lstm(model, loader_train, optimizer, device)\n","        print(\"Training loss: \", loss_train)\n","        print(\"Training metrics:\")\n","        for k, v in metrics_train.items():\n","            print(\"\\t\", k, \": \", v)\n","\n","        print(\"Validating..\")\n","        loss_val, metrics_val = validate_lstm(model, loader_val, device)\n","        print(\"Validation loss: \", loss_val)\n","        print(\"Validation metrics:\")\n","        for k, v in metrics_val.items():\n","            print(\"\\t\", k, \": \", v)\n","\n","    print(\"Done!\")"]},{"cell_type":"markdown","metadata":{"id":"rU9Nef7yal_M"},"source":["Run the experiment:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFxQxlokai6Z"},"outputs":[],"source":["train_val_loop_lstm({\n","    \"bidirectional\": True,\n","    \"batch_size\": 512,\n","    \"d_emb\": 64,\n","    \"d_hidden\": 128,\n","    \"num_epochs\": 15,\n","    \"learning_rate\": 0.005,\n","    \"l2\": 1e-6,\n","})"]},{"cell_type":"markdown","metadata":{"id":"6vA-Yjqg7n0V"},"source":["We were using bidirectional LSTMs. Please re-run the experiment with a regular (unidirectional) LSTM."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7wNrdvJ98ARB"},"outputs":[],"source":["## TODO: Re-run with unidirectional LSTMs\n","## Keep other hyperparameters fixed\n","train_val_loop_lstm({\n","    \"bidirectional\": False,\n","    \"batch_size\": 512,\n","    \"d_emb\": 64,\n","    \"d_hidden\": 128,\n","    \"num_epochs\": 15,\n","    \"learning_rate\": 0.005,\n","    \"l2\": 1e-6,\n","})\n","## END"]},{"cell_type":"markdown","metadata":{"id":"h8UChDyKaPBs"},"source":["### Questions **(2 points)**\n","\n","(a) How does the final performance of LSTMs compare to FFNNs? Is it better? What is a possible explanation?\n","\n","**TODO: Please fill in your answer here**\n","\n","(b) How does bidirectional LSTMs compare to unidirectional LSTMs? Why?\n","\n","**TODO: Please fill in your answer here**\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kMEWxkN_bpIT"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}